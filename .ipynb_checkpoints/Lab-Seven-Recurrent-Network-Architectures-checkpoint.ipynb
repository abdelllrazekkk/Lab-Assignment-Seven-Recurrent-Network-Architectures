{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment Seven: Recurrent Network Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will select a prediction task to perform on our dataset, evaluate a recurrent architecture and tune hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Members:\n",
    "1) Mohammed Ahmed Abdelrazek Aboelela\n",
    "\n",
    "2) Naim Barnett\n",
    "\n",
    "3) Katie Rink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Set : Twitter Sentimental analysis Dataset - https://www.kaggle.com/datasets/cosmos98/twitter-and-reddit-sentimental-analysis-dataset?select=Twitter_Data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview and Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the public domain of Kaggle, we import this dataset that we think is most convinient for this lab pruproses. This was a dataaset created as a part of a university Project On Sentimental Analysis On Multi-Source Social Media Platform. These tweets were made on Narendra Modi and other Leaders as well as People's opinion towards the next Prime Minister of The Nation (In Context with General Elections Held In India - 2019). This dataset consists of around 163K tweets from the social media platform Twitter, it has two cloumns, the first column has the cleaned tweets and Comments and the Second one indicates its Sentimental Label. with Sentimental Label in the form:\n",
    "\n",
    "{\n",
    "0: Indicating it is a Neutral Tweet\n",
    "\n",
    "1: Indicating a Postive Sentiment\n",
    "\n",
    "-1: Indicating a Negative Tweet\n",
    "}\n",
    "\n",
    "We can create a business goal for this dataset in the following way: say we are hired by Twitter to speed up their detection for tweets that violate policies and rules. We expect tweets that have a positive sentiment and neutral tweets in general to not be violating the policies, however, negative tweets are more likely to contain words that are inappropriate and violate the codes of the platform. Thus, this classification will help in detecting the tweets that are more likely to be violating the codes of the platform (the negative tweets), and we end up with a binary classification problem more or less.\n",
    "\n",
    "It is worth noting that we relied heavily on the professor's notebook, so everything I took from there will be cited with \"(Inspired by the professor's notebook)\" beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining and preparing our class variables. Describing the final dataset that is used for classification. Discussing methods of tokenization in our dataset as well as any decisions to force a specific length of sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 21:13:39.543990: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Importing all the needed packages\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.layers import (GRU, LSTM, Activation, Conv1D, Dense,\n",
    "                                     Dropout, Embedding, Flatten, Input,\n",
    "                                     InputLayer, MaxPooling1D, concatenate)\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 162969 entries, 0 to 162979\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   clean_text  162969 non-null  object\n",
      " 1   category    162969 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised “minimum government maximum...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162975</th>\n",
       "      <td>why these 456 crores paid neerav modi not reco...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162976</th>\n",
       "      <td>dear rss terrorist payal gawar what about modi...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162977</th>\n",
       "      <td>did you cover her interaction forum where she ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162978</th>\n",
       "      <td>there big project came into india modi dream p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162979</th>\n",
       "      <td>have you ever listen about like gurukul where ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162969 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               clean_text  category\n",
       "0       when modi promised “minimum government maximum...        -1\n",
       "1       talk all the nonsense and continue all the dra...         0\n",
       "2       what did just say vote for modi  welcome bjp t...         1\n",
       "3       asking his supporters prefix chowkidar their n...         1\n",
       "4       answer who among these the most powerful world...         1\n",
       "...                                                   ...       ...\n",
       "162975  why these 456 crores paid neerav modi not reco...        -1\n",
       "162976  dear rss terrorist payal gawar what about modi...        -1\n",
       "162977  did you cover her interaction forum where she ...         0\n",
       "162978  there big project came into india modi dream p...         0\n",
       "162979  have you ever listen about like gurukul where ...         1\n",
       "\n",
       "[162969 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Loading the Dataset\"\"\"\n",
    "data = pd.read_csv('../../Data/Twitter_Data.csv', low_memory=False)\n",
    "\n",
    "# quick cleaning\n",
    "data.dropna(inplace=True);\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "# Specifying which datatypes you expect for each column\n",
    "data['clean_text'] = data['clean_text'].astype(str)\n",
    "data['category'] = data['category'].astype(int)\n",
    "\n",
    "#Showing data\n",
    "data.info()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before replacing:\n",
      " 1    72249\n",
      " 0    55211\n",
      "-1    35509\n",
      "Name: category, dtype: int64\n",
      "================================\n",
      "After replacing:\n",
      "1    127460\n",
      "0     35509\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Turning our problem to a binary classification one to be consistent with our business case\n",
    "print(\"Before replacing:\")\n",
    "print(data[\"category\"].value_counts())\n",
    "print('================================')\n",
    "data['category'].replace([-1,0],[0,1], inplace=True)\n",
    "print(\"After replacing:\")\n",
    "print(data[\"category\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Random Tweet---------\n",
      "modi please enhance obc creamy layer 1500000 being please think over and give relaxation obcs\n",
      "================================\n",
      "Tweet Label:  1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Displaying a random tweet and its label\"\"\"\n",
    "idx = round(np.random.rand()*data.shape[0])\n",
    "print('--------Random Tweet---------')\n",
    "print(data['clean_text'][idx])\n",
    "print('================================')\n",
    "print('Tweet Label: ',data['category'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making lists of the instances and targets ready to be tokenized\n",
    "instances = data[\"clean_text\"].to_list() \n",
    "target = data[\"category\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"(Inspired by the professor's notebook)\"\"\"\n",
    "\n",
    "NUM_TOP_WORDS = None # use entire vocabulary!\n",
    "MAX_ART_LEN = 1000 # maximum and minimum number of words\n",
    "\n",
    "# tokenize the text\n",
    "tokenizer = Tokenizer(num_words=NUM_TOP_WORDS)\n",
    "tokenizer.fit_on_texts(instances)\n",
    "# save as sequences with integers replacing words\n",
    "sequences = tokenizer.texts_to_sequences(instances)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "NUM_TOP_WORDS = len(word_index) if NUM_TOP_WORDS==None else NUM_TOP_WORDS\n",
    "top_words = min((len(word_index),NUM_TOP_WORDS))\n",
    "print('Found %s unique tokens. Distilled to %d top words.' % (len(word_index),top_words))\n",
    "\n",
    "X = pad_sequences(sequences, maxlen=MAX_ART_LEN)\n",
    "\n",
    "y_ohe = keras.utils.to_categorical(target)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "print('Shape of label tensor:', y_ohe.shape)\n",
    "print(np.max(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have our final dataset that is ready for the sentiment classification task. We have our \"X\" tensor containing all the 162969 tweets, vectorized into a sequence of integers using the Keras text Tokenizer. We leave the default options of the tokenizer and the pad_sequences as we do not have massively big texts that we need to suppress or control (thanks to the limit on the character content of the tweets already imposed by Twitter :D). We also have our labels tensor as \"y_ohe\" in a one-hot encoded form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some statistics\n",
    "print(f\"Number of tweets: {len(data):,}\")\n",
    "print(f\"Number of negative tweets: {len(data[data['category'] == 0]):,}\")\n",
    "print(f\"Number of positive tweets: {len(data[data['category'] == 1]):,}\")\n",
    "\n",
    "tweet_lengths = [len(review.split()) for review in data['clean_text'].astype(str)]\n",
    "\n",
    "# print the longest review in words\n",
    "print(f\"Longest tweets in words: {max(tweet_lengths)}\")\n",
    "\n",
    "# print the 99th percentile of review lengths\n",
    "print(f\"99th percentile of tweets lengths: {np.percentile(tweet_lengths, 99)}\")\n",
    "\n",
    "# graph the number of words in each review\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_context('talk')\n",
    "sns.set_palette('colorblind')\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "sns.histplot(tweet_lengths, bins=20, kde=True)\n",
    "\n",
    "plt.title('Tweet length distribution')\n",
    "plt.xlabel('Number of words')\n",
    "plt.ylabel('Number of tweets')\n",
    "plt.xlim(0, 55)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split it into train / test subsets\n",
    "X_train, X_test, y_train_ohe, y_test_ohe = train_test_split(X, y_ohe, test_size=0.2,\n",
    "                                                            stratify=target, \n",
    "                                                            random_state=42)\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# print some stats of the data\n",
    "print(\"X_train Shape:\",X_train.shape, \"Label Shape:\", y_train_ohe.shape)\n",
    "uniq_classes = np.sum(y_train_ohe,axis=0)\n",
    "plt.bar(list(range(2)),uniq_classes)\n",
    "plt.xticks(list(range(2)), rotation='vertical')\n",
    "plt.ylabel(\"Number of Instances in Each Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing and explaining what metric we will use to evaluate your algorithm’s performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our business case that was already explained in the beginning of this notebook, it's more important for us to detect the negative tweets than detect the negative ones. In other words, having a false positive classification will be worse and less desirable than a false negative. Thus, we are looking to maximize the true positive out of the total predicted positives and consequently, the appropriate metric we decide to use is the \"Precision\" metric. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the method we will use for dividing our data for cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a kind of unbalanced data where one of the labels is significantly abundant than the other, we might be compelled to use Stratified 10-fold cross validation, to ensure that we have a good mirror of the original set and to also avoid having a significantly higher imbalance in during one set of cross-validation trainings. That's why we avoid using ShuffleSplits as we might end up having a severe imbalance during the training of one of the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10)\n",
    "skf.get_n_splits(X_train, y_train_ohe)\n",
    "print(skf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a function called create_RNN() that takes in several arguments, including a name for the RNN model, the RNN_layer_type (either 'LSTM' or 'GRU'), the number of num_units, and an embedding_layer. The function uses the RNN_layer_type argument to determine which type of RNN layer to use in the model (either LSTM or GRU). It then creates a new Sequential model with the specified name and adds the embedding_layer and RNN layer to it. The RNN layer has 100 units and uses a dropout rate of 0.2. The function then adds a dense output layer with NUM_CLASSES output units and a sigmoid activation function, and compiles the model using the 'categorical_crossentropy' loss function and the 'rmsprop' optimizer. Finally, the function returns the compiled RNN model\n",
    "\n",
    "Increasing the number of units in an RNN can allow the model to learn more complex patterns in the data, but using too many units can result in overfitting. Thus, after several attempts, I found that keeping the number of layers at around 100 prevented overfitting the best. The dropout rate is a regularization technique that helps prevent overfitting by randomly dropping out a portion of the units during training. A dropout rate of 0.2 means that 20% of the units will be dropped out during each training step. I found that any higher dropout caused generalizations to the new data to not match as well, so 20% stayed pretty consistent, while still allowing it to adapt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_RNN(name, RNN_layer_type, num_units, embedding_layer):\n",
    "    if RNN_layer_type == 'LSTM':\n",
    "        rnn = Sequential(name=name)\n",
    "        rnn.add(embedding_layer)\n",
    "        rnn.add(LSTM(100,dropout=0.2, recurrent_dropout=0.2))\n",
    "        rnn.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "        rnn.compile(loss='categorical_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "    elif RNN_layer_type == 'GRU':\n",
    "        rnn = Sequential(name=name)\n",
    "        rnn.add(embedding_layer)\n",
    "        rnn.add(LSTM(100,dropout=0.2, recurrent_dropout=0.2))\n",
    "        rnn.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "        rnn.compile(loss='categorical_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "    else:\n",
    "        raise ValueError(\"RNN_layer_type must be one of 'LSTM' or 'GRU'\")\n",
    "        \n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a function for visualizing the training and validation accuracy and loss for a Keras model. The function takes a history object as input, which is returned by the fit() method of a Keras model. The function plots the training and validation accuracy and loss. This helps understand how the model is performing during training and identify any potential overfitting or underfitting. It can also help us determine when to stop training the model to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_history(history) :\n",
    "        \n",
    "    # summarize history for accuracy\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history['accuracy'])\n",
    "    plt.plot(history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we we needed to load a pre-trained word embedding from a file and creating an embedding matrix from it. The embedding matrix is then used to initialize the embedding layer of a Keras model.\n",
    "\n",
    "Word embeddings are numerical representations of words that capture the semantic relationships between words in a text corpus. Pre-trained word embeddings, such as GloVe, are trained on large text corpora and can be used to improve the performance of natural language processing models. In our code, the glove.6B.50d.txt file is being loaded, which contains a 50-dimensional GloVe word embedding trained on a 6 billion word corpus. The code then iterates through the lines in the file and creates a dictionary that maps each word to its corresponding embedding vector.\n",
    "\n",
    "Next, the code creates an embedding matrix with the same shape as the vocabulary of the Keras model. The matrix is initialized with all zeros, and the code then iterates through the words in the vocabulary and replaces the corresponding row in the matrix with the pre-trained word embedding if the word is found in the dictionary. The resulting embedding matrix is then used to initialize the embedding layer of the Keras model. This allows the model to use the pre-trained word embeddings when learning the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 50\n",
    "# the embed size should match the file you load glove from\n",
    "embeddings_index = {}\n",
    "f = open('../../Data/glove.6B.50d.txt')\n",
    "# save key/array pairs of the embeddings\n",
    "#  the key of the dictionary is the word, the array is the embedding\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print(f\"Found {len(embeddings_index):,} word vectors.\\n\")\n",
    "\n",
    "# now fill in the matrix, using the ordering from the\n",
    "#  keras word tokenizer from before\n",
    "found_words = 0\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be ALL-ZEROS\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        found_words = found_words+1\n",
    "\n",
    "print(f\"Embedding Shape: {embedding_matrix.shape}\")\n",
    "print(f\"Total words found: {found_words:,}\")\n",
    "print(f\"Percentage: {round(100 * found_words / embedding_matrix.shape[0], 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture 1 : LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EMBED_SIZE variable defines the size of the embedding layer, which is the dimensionality of the word vectors that will be learned by the model. This value is set to 50, which means that each word will be represented by a 50-dimensional vector.\n",
    "\n",
    "The input_holder variable is an Input object that will be used as the input layer for the model. It is defined with a shape of (X_train.shape[1], ), where X_train is a 2D array of training data with X_train.shape[1] being the number of features.\n",
    "\n",
    "The embedding_layer variable is an Embedding layer that will be used in the model to map words to their corresponding word vectors. The layer is defined with the number of words in the vocabulary (len(word_index) + 1), the size of the word vectors (EMBED_SIZE), the pre-trained weights for the word vectors (weights=[embedding_matrix]), the length of the input sequences (input_length=MAX_ART_LEN), and the trainable parameter set to False, which means that the word vectors will not be updated during training.\n",
    "\n",
    "Finally, the LOAD_STORED_MODELS variable is set to True, which determines whether to load previously trained models or train new models from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 50\n",
    "input_holder = Input(shape=(X_train.shape[1], ))\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBED_SIZE,\n",
    "                            weights=[embedding_matrix],# here is the embedding getting saved\n",
    "                            input_length=MAX_ART_LEN,\n",
    "                            trainable=False)\n",
    "\n",
    "LOAD_STORED_MODELS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code appears to define two Recurrent Neural Network (RNN) models, rnn1 and rnn2, using the earlier defined create_RNN function. The rnn1 model uses a long short-term memory (LSTM) layer with 10 units, and the rnn2 model uses a LSTM layer with 25 units. Both models use the embedding_layer defined earlier as their input layer.\n",
    "\n",
    "If the LOAD_STORED_MODELS variable is True, the code attempts to load the trained weights and training history for the rnn1 and rnn2 models from the Models directory. If the models are not found or the LOAD_STORED_MODELS variable is False, the code trains the models on the X_train and y_train_ohe data for 3 epochs with a batch size of 64, and saves the trained weights and training history to the Models directory.\n",
    "\n",
    "After the models are trained, the code prints their summary, shows their training history, and calculates their confusion matrix on the X_test and y_test_ohe data using the confusion_matrix function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn1 = create_RNN('RNN-LSTM-25', 'LSTM', 10, embedding_layer)\n",
    "rnn2 = create_RNN('RNN-LSTM-50', 'LSTM', 25, embedding_layer)\n",
    "\n",
    "if LOAD_STORED_MODELS : \n",
    "    # load model 1\n",
    "    rnn1.load_weights(f\"Models/{rnn1.name}.h5\")\n",
    "    #load history\n",
    "    with open(f\"Models/{rnn1.name}.pkl\", 'rb') as file_pi:\n",
    "        history1 = pickle.load(file_pi)\n",
    "        \n",
    "    # load model 2\n",
    "    rnn2.load_weights(f\"Models/{rnn2.name}.h5\")\n",
    "    #load history\n",
    "    with open(f\"Models/{rnn2.name}.pkl\", 'rb') as file_pi:\n",
    "        history2 = pickle.load(file_pi)\n",
    "else : \n",
    "    history1 = rnn1.fit(X_train, y_train_ohe, epochs=3, batch_size=64, validation_data=(X_test, y_test_ohe))\n",
    "    # Save model\n",
    "    rnn1.save_weights(f\"Models/{rnn1.name}.h5\")\n",
    "    # save history\n",
    "    with open(f\"Models/{rnn1.name}.pkl\", 'wb') as f:\n",
    "        pickle.dump(history1.history, f)\n",
    "\n",
    "    print(f\"Model {rnn1.name} saved\")\n",
    "\n",
    "    history2 = rnn2.fit(X_train, y_train_ohe, epochs=3, batch_size=64, validation_data=(X_test, y_test_ohe))\n",
    "    # Save model\n",
    "    rnn2.save_weights(f\"Models/{rnn2.name}.h5\")\n",
    "    # save history\n",
    "    with open(f\"Models/{rnn2.name}.pkl\", 'wb') as f:\n",
    "        pickle.dump(history2.history, f)\n",
    "\n",
    "    print(f\"Model {rnn2.name} saved\")\n",
    "\n",
    "print(rnn1.summary())\n",
    "show_history(history1)\n",
    "print(confusion_matrix(X_test, y_test_ohe, rnn1))\n",
    "\n",
    "print(rnn2.summary())\n",
    "show_history(history2)\n",
    "print(confusion_matrix(X_test, y_test_ohe, rnn2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture 2 : GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Includes embedding ayer, hyper-parameters to train a total of 4 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn3 = create_RNN('RNN-GRU-25', 'GRU', 10, embedding_layer)\n",
    "rnn4 = create_RNN('RNN-GRU-50', 'GRU', 25, embedding_layer)\n",
    "\n",
    "if LOAD_STORED_MODELS : \n",
    "    # load model 3\n",
    "    rnn3.load_weights(f\"Models/{rnn3.name}.h5\")\n",
    "    #load history\n",
    "    with open(f\"Models/{rnn3.name}.pkl\", 'rb') as file_pi:\n",
    "        history3 = pickle.load(file_pi)\n",
    "    \n",
    "    \n",
    "    # load model 4\n",
    "    rnn4.load_weights(f\"Models/{rnn4.name}.h5\")\n",
    "    #load history\n",
    "    with open(f\"Models/{rnn4.name}.pkl\", 'rb') as file_pi:\n",
    "        history4 = pickle.load(file_pi)\n",
    "else : \n",
    "    history3 = rnn3.fit(X_train, y_train_ohe, epochs=3, batch_size=64, validation_data=(X_test, y_test_ohe))\n",
    "    # Save model\n",
    "    rnn3.save_weights(f\"Models/{rnn3.name}.h5\")\n",
    "    # save history\n",
    "    with open(f\"Models/{rnn3.name}.pkl\", 'wb') as f:\n",
    "        pickle.dump(history3.history, f)\n",
    "\n",
    "    print(f\"Model {rnn3.name} saved\")\n",
    "\n",
    "    history4 = rnn4.fit(X_train, y_train_ohe, epochs=3, batch_size=64, validation_data=(X_test, y_test_ohe))\n",
    "    # Save model\n",
    "    rnn4.save_weights(f\"Models/{rnn4.name}.h5\")\n",
    "    # save history\n",
    "    with open(f\"Models/{rnn4.name}.pkl\", 'wb') as f:\n",
    "        pickle.dump(history4.history, f)\n",
    "\n",
    "    print(f\"Model {rnn4.name} saved\")\n",
    "\n",
    "print(rnn3.summary())\n",
    "show_history(history3)\n",
    "print(confusion_matrix(X_test, y_test_ohe, rnn3))\n",
    "\n",
    "print(rnn4.summary())\n",
    "show_history(history4)\n",
    "print(confusion_matrix(X_test, y_test_ohe, rnn4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Second Recurrent Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the best RNN parameters and architecture, add a second recurrent chain to your RNN. The input to the second chain should be the output sequence of the first chain. Visualize the performance of training and validation sets versus the training iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RNN(name, RNN_layer_type, num_units, embedding_layer):\n",
    "    if RNN_layer_type == 'LSTM':\n",
    "        rnn = Sequential(name=name)\n",
    "        rnn.add(embedding_layer)\n",
    "        rnn.add(LSTM(50,dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "        rnn.add(LSTM(50,dropout=0.2, recurrent_dropout=0.2))\n",
    "        rnn.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "        rnn.compile(loss='categorical_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "    elif RNN_layer_type == 'GRU':\n",
    "        rnn = Sequential(name=name)\n",
    "        rnn.add(embedding_layer)\n",
    "        rnn.add(GRU(50,dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "        rnn.add(GRU(50,dropout=0.2, recurrent_dropout=0.2))\n",
    "        rnn.add(Dense(NUM_CLASSES, activation='sigmoid'))\n",
    "        rnn.compile(loss='categorical_crossentropy', \n",
    "              optimizer='rmsprop', \n",
    "              metrics=['accuracy'])\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn5 = create_RNN('RNN-GRU-25-2Chain', 'GRU', 25, embedding_layer)\n",
    "\n",
    "if LOAD_STORED_MODELS : \n",
    "    # load model 5\n",
    "    rnn5.load_weights(f\"Models/{rnn5.name}.h5\")\n",
    "    #load history\n",
    "    with open(f\"Models/{rnn5.name}.pkl\", 'rb') as file_pi:\n",
    "        history5 = pickle.load(file_pi)\n",
    "else : \n",
    "    history5 = rnn5.fit(X_train, y_train_ohe, epochs=3, batch_size=2048, validation_data=(X_test, y_test_ohe))\n",
    "    # Save model\n",
    "    rnn5.save_weights(f\"Models/{rnn5.name}.h5\")\n",
    "    # save history\n",
    "    with open(f\"Models/{rnn5.name}.pkl\", 'wb') as f:\n",
    "        pickle.dump(history5.history, f)\n",
    "\n",
    "    print(f\"Model {rnn5.name} saved\")\n",
    "\n",
    "print(rnn5.summary())\n",
    "show_history(history5)\n",
    "print(confusion_matrix(X_test, y_test_ohe, rnn5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare all RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the method of train/test splitting and evaluation criteria that you argued for at the beginning of the lab. Visualize the results of all the RNNs you trained.  Use proper statistical comparison techniques to determine which method(s) is (are) superior.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work (1 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One idea (required for 7000 level students to do one of these options):                                                                         Option 1: Use dimensionality reduction (choose an appropriate method from this list: t-SNE, SVD, PCA, or UMAP) to visualize the word embeddings of a subset of words in your vocabulary that you expect to have an analogy that can be captured by the embedding. Try to interpret if an analogy exists, show the vectors that support/refute the analogy, and interpret your findings.                                                                                                                                         Options 2: Use the ConceptNet Numberbatch embedding and compare to GloVe. Which method is better for your specific application? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Idea (NOT required): Try to create a RNN for generating novel text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You have free rein to provide additional analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 tensorflow",
   "language": "python",
   "name": "condaenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
