{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assignment Seven: Recurrent Network Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will select a prediction task to perform on our dataset, evaluate a recurrent architecture and tune hyper-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Team Members:\n",
    "1) Mohammed Ahmed Abdelrazek Aboelela\n",
    "\n",
    "2) Naim Barnett\n",
    "\n",
    "3) Katie Rink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Set : Twitter Sentimental analysis Dataset - https://www.kaggle.com/datasets/cosmos98/twitter-and-reddit-sentimental-analysis-dataset?select=Twitter_Data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview and Business Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the public domain of Kaggle, we import this dataset that we think is most convinient for this lab pruproses. This was a dataaset created as a part of a university Project On Sentimental Analysis On Multi-Source Social Media Platform. These tweets were made on Narendra Modi and other Leaders as well as People's opinion towards the next Prime Minister of The Nation (In Context with General Elections Held In India - 2019). This dataset consists of around 163K tweets from the social media platform Twitter, it has two cloumns, the first column has the cleaned tweets and Comments and the Second one indicates its Sentimental Label. with Sentimental Label in the form:\n",
    "\n",
    "{\n",
    "0: Indicating it is a Neutral Tweet\n",
    "\n",
    "1: Indicating a Postive Sentiment\n",
    "\n",
    "-1: Indicating a Negative Tweet\n",
    "}\n",
    "\n",
    "We can create a business goal for this dataset in the following way: say we are hired by Twitter to speed up their detection for tweets that violate policies and rules. We expect tweets that have a positive sentiment and neutral tweets in general to not be violating the policies, however, negative tweets are more likely to contain words that are inappropriate and violate the codes of the platform. Thus, this classification will help in detecting the tweets that are more likely to be violating the codes of the platform (the negative tweets), and we end up with a binary classification problem more or less.\n",
    "\n",
    "It is worth noting that we relied heavily on the professor's notebook, so everything I took from there will be cited with \"(Inspired by the professor's notebook)\" beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining and preparing our class variables. Describing the final dataset that is used for classification. Discussing methods of tokenization in our dataset as well as any decisions to force a specific length of sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/applications/anaconda/3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/hpc/applications/anaconda/3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/hpc/applications/anaconda/3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/hpc/applications/anaconda/3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/hpc/applications/anaconda/3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/hpc/applications/anaconda/3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Importing all the needed packages\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.layers import (GRU, LSTM, Activation, Conv1D, Dense,\n",
    "                                     Dropout, Embedding, Flatten, Input,\n",
    "                                     InputLayer, MaxPooling1D, concatenate)\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 162969 entries, 0 to 162979\n",
      "Data columns (total 2 columns):\n",
      "clean_text    162969 non-null object\n",
      "category      162969 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.7+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>when modi promised “minimum government maximum...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>talk all the nonsense and continue all the dra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what did just say vote for modi  welcome bjp t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>asking his supporters prefix chowkidar their n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>answer who among these the most powerful world...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162975</th>\n",
       "      <td>why these 456 crores paid neerav modi not reco...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162976</th>\n",
       "      <td>dear rss terrorist payal gawar what about modi...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162977</th>\n",
       "      <td>did you cover her interaction forum where she ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162978</th>\n",
       "      <td>there big project came into india modi dream p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162979</th>\n",
       "      <td>have you ever listen about like gurukul where ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>162969 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               clean_text  category\n",
       "0       when modi promised “minimum government maximum...        -1\n",
       "1       talk all the nonsense and continue all the dra...         0\n",
       "2       what did just say vote for modi  welcome bjp t...         1\n",
       "3       asking his supporters prefix chowkidar their n...         1\n",
       "4       answer who among these the most powerful world...         1\n",
       "...                                                   ...       ...\n",
       "162975  why these 456 crores paid neerav modi not reco...        -1\n",
       "162976  dear rss terrorist payal gawar what about modi...        -1\n",
       "162977  did you cover her interaction forum where she ...         0\n",
       "162978  there big project came into india modi dream p...         0\n",
       "162979  have you ever listen about like gurukul where ...         1\n",
       "\n",
       "[162969 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Loading the Dataset\"\"\"\n",
    "data = pd.read_csv('../../Data/Twitter_Data.csv', low_memory=False)\n",
    "\n",
    "# quick cleaning\n",
    "data.dropna(inplace=True);\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "# Specifying which datatypes you expect for each column\n",
    "data['clean_text'] = data['clean_text'].astype(str)\n",
    "data['category'] = data['category'].astype(int)\n",
    "\n",
    "#Showing data\n",
    "data.info()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before replacing:\n",
      " 1    72249\n",
      " 0    55211\n",
      "-1    35509\n",
      "Name: category, dtype: int64\n",
      "================================\n",
      "After replacing:\n",
      "1    127460\n",
      "0     35509\n",
      "Name: category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Turning our problem to a binary classification one to be consistent with our business case\n",
    "print(\"Before replacing:\")\n",
    "print(data[\"category\"].value_counts())\n",
    "print('================================')\n",
    "data['category'].replace([-1,0],[0,1], inplace=True)\n",
    "print(\"After replacing:\")\n",
    "print(data[\"category\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Random Tweet---------\n",
      "who the enemy bjp antisatellite \n",
      "================================\n",
      "Tweet Label:  1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Displaying a random tweet and its label\"\"\"\n",
    "idx = round(np.random.rand()*data.shape[0])\n",
    "print('--------Random Tweet---------')\n",
    "print(data['clean_text'][idx])\n",
    "print('================================')\n",
    "print('Tweet Label: ',data['category'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making lists of the instances and targets ready to be tokenized\n",
    "instances = data[\"clean_text\"].to_list() \n",
    "target = data[\"category\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 113678 unique tokens. Distilled to 113678 top words.\n",
      "Shape of data tensor: (162969, 52)\n",
      "Shape of label tensor: (162969, 2)\n",
      "113678\n"
     ]
    }
   ],
   "source": [
    "\"\"\"(Inspired by the professor's notebook)\"\"\"\n",
    "\n",
    "NUM_TOP_WORDS = None # use entire vocabulary!\n",
    "MAX_ART_LEN = 1000 # maximum and minimum number of words\n",
    "\n",
    "# tokenize the text\n",
    "tokenizer = Tokenizer(num_words=NUM_TOP_WORDS)\n",
    "tokenizer.fit_on_texts(instances)\n",
    "# save as sequences with integers replacing words\n",
    "sequences = tokenizer.texts_to_sequences(instances)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "NUM_TOP_WORDS = len(word_index) if NUM_TOP_WORDS==None else NUM_TOP_WORDS\n",
    "top_words = min((len(word_index),NUM_TOP_WORDS))\n",
    "print('Found %s unique tokens. Distilled to %d top words.' % (len(word_index),top_words))\n",
    "\n",
    "X = pad_sequences(sequences)\n",
    "\n",
    "y_ohe = keras.utils.to_categorical(target)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "print('Shape of label tensor:', y_ohe.shape)\n",
    "print(np.max(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have our final dataset that is ready for the sentiment classification task. We have our \"X\" tensor containing all the 162969 tweets, vectorized into a sequence of integers using the Keras text Tokenizer. We leave the default options of the tokenizer and the pad_sequences as we do not have massively big texts that we need to suppress or control (thanks to the limit on the character content of the tweets already imposed by Twitter :D). We also have our labels tensor as \"y_ohe\" in a one-hot encoded form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Shape: (130375, 52) Label Shape: (130375, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split it into train / test subsets\n",
    "X_train, X_test, y_train_ohe, y_test_ohe = train_test_split(X, y_ohe, test_size=0.2,\n",
    "                                                            stratify=target, \n",
    "                                                            random_state=42)\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# print some stats of the data\n",
    "print(\"X_train Shape:\",X_train.shape, \"Label Shape:\", y_train_ohe.shape)\n",
    "uniq_classes = np.sum(y_train_ohe,axis=0)\n",
    "plt.bar(list(range(2)),uniq_classes)\n",
    "plt.xticks(list(range(2)), rotation='vertical')\n",
    "plt.ylabel(\"Number of Instances in Each Class\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing and explaining what metric we will use to evaluate your algorithm’s performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our business case that was already explained in the beginning of this notebook, it's more important for us to detect the negative tweets than detect the negative ones. In other words, having a false positive classification will be worse and less desirable than a false negative. Thus, we are looking to maximize the true positive out of the total predicted positives and consequently, the appropriate metric we decide to use is the \"Precision\" metric. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the method we will use for dividing our data for cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a kind of unbalanced data where one of the labels is significantly abundant than the other, we might be compelled to use Stratified 10-fold cross validation, to ensure that we have a good mirror of the original set and to also avoid having a significantly higher imbalance in during one set of cross-validation trainings. That's why we avoid using ShuffleSplits as we might end up having a severe imbalance during the training of one of the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=10, random_state=None, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=10)\n",
    "skf.get_n_splits(X_train, y_train_ohe)\n",
    "print(skf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[3 points] Investigate at least two different recurrent network architectures (perhaps LSTM and GRU). Be sure to use an embedding layer (pre-trained, from scratch, OR both). Adjust hyper-parameters of the networks as needed to improve generalization performance (train a total of at least four models). Discuss the performance of each network and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def create_RNN(name, RNN_layer_type, num_units):\n",
    "    EMBED_SIZE = 50\n",
    "    RNN_STATESIZE = 100\n",
    "    rnns = []\n",
    "    input_holder = Input(shape=(X_train.shape[1], ))\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBED_SIZE,\n",
    "                            weights=[embedding_matrix],# here is the embedding getting saved\n",
    "                            input_length=MAX_ART_LEN,\n",
    "                            trainable=False)\n",
    "    \n",
    "    if RNN_layer_type == 'LSTM':\n",
    "        RNN_layer = LSTM\n",
    "        x = LSTM(100, dropout=0.2, recurrent_dropout=0.2)(embedding_layer)\n",
    "        x = Dense(NUM_CLASSES, activation='sigmoid')(x)\n",
    "        rnn = Model(inputs=input_holder,outputs=x)\n",
    "    elif RNN_layer_type == 'GRU':\n",
    "        x = GRU(100, dropout=0.2, recurrent_dropout=0.2)(embedding_layer)\n",
    "        x = Dense(NUM_CLASSES, activation='sigmoid')(x)\n",
    "        rnn = Model(inputs=input_holder,outputs=x)\n",
    "    else:\n",
    "        raise ValueError(\"RNN_layer_type must be one of 'LSTM' or 'GRU'\")\n",
    "        \n",
    "    opt = Adam(lr=0.0001, epsilon=0.0001, clipnorm=1.0)\n",
    "    rnn.compile(loss='binary_crossentropy', \n",
    "              optimizer= opt, \n",
    "              metrics=['accuracy'])\n",
    "        \n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_history(history) :\n",
    "        \n",
    "    # summarize history for accuracy\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history['accuracy'])\n",
    "    plt.plot(history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "    # summarize history for loss\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history['loss'])\n",
    "    plt.plot(history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400,000 word vectors.\n",
      "\n",
      "Embedding Shape: (113679, 100)\n",
      "Total words found: 40,507\n",
      "Percentage: 35.63\n"
     ]
    }
   ],
   "source": [
    "EMBED_SIZE = 100\n",
    "# the embed size should match the file you load glove from\n",
    "embeddings_index = {}\n",
    "f = open('../../Data/glove.6B.100d.txt')\n",
    "# save key/array pairs of the embeddings\n",
    "#  the key of the dictionary is the word, the array is the embedding\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print(f\"Found {len(embeddings_index):,} word vectors.\\n\")\n",
    "\n",
    "# now fill in the matrix, using the ordering from the\n",
    "#  keras word tokenizer from before\n",
    "found_words = 0\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be ALL-ZEROS\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        found_words = found_words+1\n",
    "\n",
    "print(f\"Embedding Shape: {embedding_matrix.shape}\")\n",
    "print(f\"Total words found: {found_words:,}\")\n",
    "print(f\"Percentage: {round(100 * found_words / embedding_matrix.shape[0], 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture 1 : LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Includes embedding ayer, hyper-parameters to train a total of 4 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MAX_ART_LEN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-a0d03b0290c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrnn1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_RNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RNN-LSTM-25'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LSTM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_ohe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_ohe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mshow_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrnn2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_RNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RNN-LSTM-50'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LSTM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-91c5028ecf48>\u001b[0m in \u001b[0;36mcreate_RNN\u001b[0;34m(name, RNN_layer_type, num_units)\u001b[0m\n\u001b[1;32m     10\u001b[0m                             \u001b[0mEMBED_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                             \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m# here is the embedding getting saved\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                             \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_ART_LEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                             trainable=False)\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MAX_ART_LEN' is not defined"
     ]
    }
   ],
   "source": [
    "rnn1 = create_RNN('RNN-LSTM-25', 'LSTM', 25)\n",
    "print(rnn1.summary())\n",
    "history = rnn1.fit(X_train, y_train_ohe, epochs=3, batch_size=64, validation_data=(X_test, y_test_ohe))\n",
    "show_history(history)\n",
    "rnn2 = create_RNN('RNN-LSTM-50', 'LSTM', 50)\n",
    "print(rnn2.summary())\n",
    "history = rnn2.fit(X_train, y_trai_ohe, epochs=3, batch_size=64, validation_data=(X_test, y_test_ohe))\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture 2 : GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Includes embedding ayer, hyper-parameters to train a total of 4 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn1 = create_RNN('RNN-GRU-25', 'GRU', 25)\n",
    "history = rnn1.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe), epochs=25, batch_size=2048, verbose=1)\n",
    "print(rnn1.summary())\n",
    "show_history(history)\n",
    "rnn2 = create_RNN('RNN-GRU-50', 'GRU', 50)\n",
    "history = rnn2.fit(X_train, y_train_ohe, validation_data=(X_test, y_test_ohe), epochs=25, batch_size=2048, verbose=1)\n",
    "print(rnn2.summary())\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding a Second Recurrent Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the best RNN parameters and architecture, add a second recurrent chain to your RNN. The input to the second chain should be the output sequence of the first chain. Visualize the performance of training and validation sets versus the training iterations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare all RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the method of train/test splitting and evaluation criteria that you argued for at the beginning of the lab. Visualize the results of all the RNNs you trained.  Use proper statistical comparison techniques to determine which method(s) is (are) superior.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exceptional Work (1 points total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One idea (required for 7000 level students to do one of these options):                                                                         Option 1: Use dimensionality reduction (choose an appropriate method from this list: t-SNE, SVD, PCA, or UMAP) to visualize the word embeddings of a subset of words in your vocabulary that you expect to have an analogy that can be captured by the embedding. Try to interpret if an analogy exists, show the vectors that support/refute the analogy, and interpret your findings.                                                                                                                                         Options 2: Use the ConceptNet Numberbatch embedding and compare to GloVe. Which method is better for your specific application? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Idea (NOT required): Try to create a RNN for generating novel text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You have free rein to provide additional analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
